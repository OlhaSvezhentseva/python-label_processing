{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KYkbCXTcDPwq"
      },
      "source": [
        "# TensorFlow Classifier Tutorial"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cw-g4QBYls6M"
      },
      "source": [
        "The TensorFlow Sequential Classifier is a machine learning model implemented using the Sequential API in TensorFlow, built with the Keras neural networks API. It is designed for the task of classification, which involves assigning predefined labels or categories to input data."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ONI_50YJeyCE"
      },
      "source": [
        "https://www.tensorflow.org/tutorials/images/classification"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bS5J4XAhd2Ac"
      },
      "source": [
        "# Import librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcglOWaRZ6sq"
      },
      "outputs": [],
      "source": [
        "# Third-Party Librairies\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import glob, os\n",
        "import pathlib\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "import re\n",
        "\n",
        "# TensorFlow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n",
        "# Google Colab - if want to train your model in Google Colab using the Google GPU\n",
        "from google.colab import drive"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1oU_jGT7eGy8"
      },
      "source": [
        "# Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('Path to your training data repository')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hrz26oV-eYOQ"
      },
      "source": [
        "# Import Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnyQUdEVj1AK"
      },
      "outputs": [],
      "source": [
        "data_dir = pathlib.Path('Path to the pictures repository')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JM_MKjUMjpEL"
      },
      "outputs": [],
      "source": [
        "images = glob.glob('*/*.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zHI_dFUirQ3",
        "outputId": "ad67f828-d96f-4ecb-b614-d956ff7ae3b2"
      },
      "outputs": [],
      "source": [
        "image_count = len(list(glob.glob('handwritten/*.jpg'))) # Example: all the JPGs in the \"handwritten\" folder, also belonging to that class\n",
        "print(image_count) # Number of pictures in the folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8lsZdJNdSmw",
        "outputId": "76d9a8b3-f6e5-45d6-dcb7-a3c21b0f20ef"
      },
      "outputs": [],
      "source": [
        "image_count = len(list(glob.glob('printed/*.jpg')))\n",
        "print(image_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aU2elUhJdZNM",
        "outputId": "807b3bd6-390a-489b-d336-d7c366916177"
      },
      "outputs": [],
      "source": [
        "image_count = len(list(glob.glob('Path to the pictures use for testing/*.jpg')))\n",
        "print(image_count)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tKVQqZZQefQz"
      },
      "source": [
        "# Prepare model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyqcNBzggQAV"
      },
      "outputs": [],
      "source": [
        "# Define constants for batch size, image height, and image width\n",
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pu3yYj0jkgT",
        "outputId": "bff030f4-b2c4-452f-975f-fe28a9aac68a"
      },
      "outputs": [],
      "source": [
        "# Create a training dataset using image_dataset_from_directory\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j3hpFYGjki_",
        "outputId": "3ef8a7ff-77c8-4ed6-c5f0-57c6a8476534"
      },
      "outputs": [],
      "source": [
        "# Create a validation dataset using image_dataset_from_directory\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory\n",
        "(data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDj_H4SVkeP0",
        "outputId": "0a55b0c7-c31b-452d-a230-2202df3a4be3"
      },
      "outputs": [],
      "source": [
        "# Get the class names from the training dataset\n",
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "HQ5LIV2-keSb",
        "outputId": "43a320ad-365b-4527-db0b-70c656d5a01f"
      },
      "outputs": [],
      "source": [
        "# Visualize a sample of images from the training dataset\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rllkkI0ukeVT",
        "outputId": "3d23ace3-0e99-484a-eae1-f73af5e0bd1d"
      },
      "outputs": [],
      "source": [
        "# Print the shape of image and label batches from the training dataset\n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wART1FaAeqrv"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu6LF4ugkeX4"
      },
      "outputs": [],
      "source": [
        "# Define AUTOTUNE for dynamic dataset tuning\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Apply caching, shuffling, and prefetching to the training dataset\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Apply caching and prefetching to the validation dataset\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-SwRHXgtGMr"
      },
      "outputs": [],
      "source": [
        "# Create a normalization layer to rescale pixel values to [0,1]\n",
        "normalization_layer = layers.Rescaling(1./255)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TN-KOYF4tGPx",
        "outputId": "9a47c596-a7f9-492a-9292-4b2e69836241"
      },
      "outputs": [],
      "source": [
        "# Apply normalization to the training dataset using the map function\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "\n",
        "# Retrieve a batch of normalized images and labels\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "first_image = image_batch[0]\n",
        "\n",
        "# Print the minimum and maximum pixel values after normalization\n",
        "print(np.min(first_image), np.max(first_image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj6jk_mstOBz"
      },
      "outputs": [],
      "source": [
        "# Determine the number of classes based on the class names\n",
        "num_classes = len(class_names)\n",
        "\n",
        "# Build a sequential model for image classification\n",
        "model = Sequential([\n",
        "  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),\n",
        "  layers.Conv2D(16, 3, padding='same', activation='elu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='elu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='elu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='elu'),\n",
        "  layers.Dense(num_classes)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKkStVlttOLS",
        "outputId": "6b6794c6-0091-4919-ea5e-d18ce6e67ea3"
      },
      "outputs": [],
      "source": [
        "# Display the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2HFW1xog7tq"
      },
      "outputs": [],
      "source": [
        "# Compile the model with Adam optimizer, sparse categorical crossentropy loss, and accuracy metric\n",
        "model.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVH0x8TMtGSc",
        "outputId": "5b458d7c-44fb-4045-930f-6bb456241a51"
      },
      "outputs": [],
      "source": [
        "# Set the number of training epochs\n",
        "epochs=32\n",
        "\n",
        "# Train the model using the training dataset and validate on the validation dataset\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "id": "O0lS-tdktGVm",
        "outputId": "a7bc858e-9f6e-439a-effd-44d82dbe7e58"
      },
      "outputs": [],
      "source": [
        "# Extract training history for accuracy and loss\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "# Create a plot to visualize training and validation accuracy\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "# Create a plot to visualize training and validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n",
        "\n",
        "# Extract the last accuracy and loss values for reporting\n",
        "acc_last =acc*100\n",
        "print(\"Final loss:\", loss[-1])\n",
        "print(\"Final accuracy:\", acc_last[-1])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ErlCue7teuja"
      },
      "source": [
        "# Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdI4ynqWa6yj"
      },
      "outputs": [],
      "source": [
        "model.save('Path where you want to save the model in your Drive')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ItvR8O_Ok0gf"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5MNcgJUbgBQ",
        "outputId": "7c914073-418e-4c9f-e2a1-7e6c1bf6280f"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('Path where the model is saved in the Drive')\n",
        "\n",
        "# Check its architecture\n",
        "model.summary()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j_XzhoH8k4gu"
      },
      "source": [
        "# Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AEOc61f-gOLF",
        "outputId": "d3fcd8bd-9da6-457f-b76f-168a3548afa1"
      },
      "outputs": [],
      "source": [
        "# Create a list to store all predictions\n",
        "all_predictions = []\n",
        "print(\"Predicting classes\")\n",
        "\n",
        "# Iterate over all image files in the specified directory\n",
        "for file in glob.glob(f\"Path to the pictures use of testing/*.jpg\"):\n",
        "    # Specify the path to your image\n",
        "    image = tf.keras.utils.load_img(file, target_size=(img_height, img_width))\n",
        "    img_array = tf.keras.utils.img_to_array(image)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "    # Make predictions using the trained model\n",
        "    predictions = model.predict(img_array)\n",
        "    score = tf.nn.softmax(predictions[0])\n",
        "    # Extract information about the prediction\n",
        "    entry = {}\n",
        "    entry['filename'] = os.path.basename(file)\n",
        "    entry['class'] = class_names[np.argmax(score)]\n",
        "    entry['score'] = 100 * np.max(score)\n",
        "    # Append the prediction to the list\n",
        "    all_predictions.append(entry)\n",
        "    # Create a DataFrame from the list of predictions\n",
        "    df = pd.DataFrame(all_predictions)\n",
        "    # Save the DataFrame to a CSV file\n",
        "    df.to_csv('prediction_classifer.csv')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k4kTnXI4lfGu"
      },
      "source": [
        "# Save Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4dMpOVmuQPR"
      },
      "outputs": [],
      "source": [
        "def create_dirs(dataframe: pd.Dataframe, path: str) -> None:\n",
        "    \"\"\"\n",
        "    Creates for every class a seperate directory.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.Dataframe): dataframe containing the classes as a column\n",
        "        path (str): path of chosen directory\n",
        "    \"\"\"\n",
        "    uniques = dataframe[\"class\"].unique()\n",
        "    for uni_class in uniques:\n",
        "        Path(f\"{path}/{uni_class}\").mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDsYIRxp0DqN"
      },
      "outputs": [],
      "source": [
        "def load_jpg(filepath: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Loads the jpg files using the opencv module.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): path to jpg files\n",
        "\n",
        "    Returns:\n",
        "        Mat (numpy.typing.NDArray): cv2 image object\n",
        "    \"\"\"\n",
        "    jpg = cv2.imread(filepath)\n",
        "    return jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTqtW3YgpF-V"
      },
      "outputs": [],
      "source": [
        "def make_file_name(label_id: str, pic_class: str, occurence: int) -> None:\n",
        "    \"\"\"\n",
        "    Creates a fitting filename.\n",
        "\n",
        "    Args:\n",
        "        label_id (str): string containing the label id.\n",
        "        pic_class (str): class of the label.\n",
        "        occurence (int): counts how many times the label class already\n",
        "                         occured in the picture.\n",
        "    \"\"\"\n",
        "    label_id = re.sub(r\"_+label\", \"\", label_id)\n",
        "    filename = f\"{label_id}_label_{pic_class}_{occurence}.jpg\"\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3CT-JxFpc4_"
      },
      "outputs": [],
      "source": [
        "def crop_picture(img_raw: np.ndarray , path: str,\n",
        "                 filename: str, pic_class: str) -> None:\n",
        "    \"\"\"\n",
        "    Crops the picture using the given coordinates.\n",
        "\n",
        "    Args:\n",
        "        img_raw (numpy.matrix): input jpg converted to numpy matrix by cv2.\n",
        "        path (str): path where the picture should be saved.\n",
        "        filename (str): name of the picture.\n",
        "        pic_class (str): class of the label.\n",
        "    \"\"\"\n",
        "    filepath=f\"{path}/{pic_class}/{filename}\"\n",
        "    cv2.imwrite(filepath, img_raw)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8A0hDzHqh1l"
      },
      "outputs": [],
      "source": [
        "# Specify the path to the repository of pictures used for testing\n",
        "dir_path = 'Path to the repository of the pictures used for Testing'\n",
        "# Specify the path where you want the outputs to be saved in the Drive\n",
        "out_dir = 'Path where you want the outputs to be saved in the Drive'\n",
        "\n",
        "# Check if the directory path ends with a \"/\"\n",
        "if dir_path[-1] == \"/\":\n",
        "    # If true, use the parent directory name followed by '_classified'\n",
        "    new_dir = f\"{os.path.basename(os.path.dirname(dir_path))}_classified\"\n",
        "else:\n",
        "    # If false, use the directory name followed by '_classified'\n",
        "    new_dir = f\"{os.path.basename(dir_path)}_classified\"\n",
        "\n",
        "# Create the full path for the output directory\n",
        "path = f\"{out_dir}/{new_dir}/\"\n",
        "# Create the directory if it doesn't exist, including parent directories\n",
        "Path(path).mkdir(parents=True, exist_ok=True)\n",
        "# Create directories for every class based on the DataFrame 'df'\n",
        "create_dirs(df, path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tx5ylarRriIl"
      },
      "outputs": [],
      "source": [
        "# Iterate over all JPG files in the specified directory\n",
        "for filepath in glob.glob(os.path.join(dir_path, '*.jpg')):\n",
        "    # Extract the filename from the full filepath\n",
        "    filename = os.path.basename(filepath)\n",
        "    # Find matching rows in the DataFrame 'df' for the current filename\n",
        "    match = df[df.filename == filename]\n",
        "    # Load the JPG image using the 'load_jpg' function\n",
        "    image_raw = load_jpg(filepath)\n",
        "    # Extract the stem of the filename to use as 'label_id'\n",
        "    label_id = Path(filename).stem\n",
        "    # Initialize an empty list to store classes for each match\n",
        "    classes = []\n",
        "    # Iterate over each row in the DataFrame 'match'\n",
        "    for _, row in match.iterrows():\n",
        "        # Extract the predicted class from the current row\n",
        "        pic_class = row['class']\n",
        "        # Count occurrences of the current class in the 'classes' list\n",
        "        occ = classes.count(pic_class) + 1\n",
        "        # Generate a new filename using 'make_file_name' function\n",
        "        filename = make_file_name(label_id, pic_class, occ)\n",
        "        # Crop and save the image using 'crop_picture' function\n",
        "        crop_picture(image_raw, path, filename, pic_class)\n",
        "# Print a message indicating successful image saving\n",
        "print(f\"\\nThe images have been successfully saved in {os.path.join(out_dir, new_dir)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
