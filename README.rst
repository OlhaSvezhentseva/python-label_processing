label_Processing 0.0.2
===================================================================

*A Python package for the Berlin Natural History Museum*

.. contents ::

Introduction
------------
This package contains three segmentation models and functions to handle
segmenting and performing an OCR on entomology specimen labels. Its installation also includes 
scripts designed for classifying the labels during segmentation and preprocessing the images before applying the Pytesseract or Google Vision OCRs.


Installation
------------
1. Clone python-mfnb from https://gitlab.com/preuss.leonardo/python-label_processing.git .

   `git clone https://gitlab.com/preuss.leonardo/python-label_processing.git`

2. cd in label_processing-main

   `cd <path to python-label_processing-main>`
   
3. Install with pip, which will automatically fetch the requirements if
   you don't have it already.

   `pip install .`


Modules
-------
* segmentation_cropping
   Module containing all functions concerning the application of the segmentation 
   models, the classification of the cropped labels and the use of the predicted coordinates for cropping the labels.  


* text_recognition
   Module containing the Pytesseract OCR parameters and image preprocessing to be performed on the _cropped jpg outputs from
   the segmentation_cropping.py module.


* vision
   Module to call the Google Vision OCR API to be performed on the _cropped jpg outputs from
   the segmentation_cropping.py module.


* utils
   Module containing general functions: raise an error if the _cropped jpg are not found in the given path, 
   generate a new filename and directory for the preprocessed images generated by Pytesseract, save the json files of 
   the transcription outputs generated by Google Visionb (vision module) and Pytesseract (text_recognition module), check and correct 
   the NURI transcriptions in the json files to have a clean output.


* redundancy
   Check the redundancy of a given transcription (Ground Truth or OCR generated).


Scripts
-------
For usage information, run any of these scripts with the option --help.

* crop_seg.py
   Uses a segmentation-model to perform segmentation on jpg images, crop and classify them.

   **Inputs:**
      - the path to the directory of the input jpgs (jpg_dir)
      - the model used for the segmentation (model)
      - the path to the directory in which the resulting crops and the csv will be stored (out_dir)

   **Outputs:**
      - the labels in the pictures are segmented and cropped out of the picture, becoming their own file named after their jpg of origin and    assigned class.
      - the predicted segmentation outputs are also saved as a csv (filename, class, prediction score, coordinates).

* tesseract_ocr.py
   Performs Pytesseract on the segmented labels and returns it as a json file. 
   Before the ocr, preprocessing is done on the pictures to enhance the results.

   **Inputs:**
      - the path to the directory of the input jpgs (crop_dir)
      - optional argument: select whether the verbose should be visible or not

   **Outputs:**
      - ocr results as a json file
      - preprocessed images

* vision_api.py
   Performs the Google Vision OCR on the segmented labels by calling the API and returns it as a json file. 
   
   **Inputs:**
      - the path to the google credentials json file (credentials) 
      - the path to the directory of the input jpgs (crop_dir)

   **Output:**
      - ocr results as a json file

* label_redundancy.py
   Module calculating labels' redundancy of a given text transcription (Ground Truth or OCR generated).
   
   **Input:**
      - the path to the transcription dataset

   **Output:**
      - redundancy percentage of the dataset


Input preparation
-----------------
**The modules are best to be performed on jpg images of labels from entomology databases such as:**
   - `AntWeb`_
   - `Bees&Bytes`_
   - LEP_PHIL - pictures of specimens from the Philippines (by Th√©o Leger)
   - `Atlas of Living Australia`_


**In terms of data acquisition, the following standards are recommended to optimize the outputs:**

- The pictures quality should be standardized and uniform as much as possible, preferably using macro photography, the .jpg format and    300 DPI resolution.
- If there are multiple labels in one picture, they should be clearly separated from one another without overlapping. The text in the label should be aligned horizontally.
- If possible, the specimen shouldn't be present in the picture with the labels.
- If the labels in the different pictures are similar (same colours and/or same nature/content), they should always be placed the same way at the same spot from one picture to another. *ex: label with location always bottom right, collection number top left, taxonomy top right etc...*
- A black background like in LEP_PHIL is prefered, but a white background is also acceptable.


.. _AntWeb: https://www.antweb.org/
.. _Bees&Bytes: https://www.zooniverse.org/projects/mfnberlin/bees-and-bytes  
.. _Atlas of Living Australia: https://www.ala.org.au/


Setting up google cloud vision API and getting credentials
----------------------------------------------------------
- In order to use the google API you need to create a Google account and set it up for Vision.
- How to setup your Google Cloud Vision is explained `on the website`_.
- You then need to retrieve your credentials json (everything is explained in the provided link).
- The credentials json file should then be provided as an input in the `vision.py` script.


Installing Pytesseract for MacOS
--------------------------------
Informations about Pytesseract can be found `here`_ or `this website`_.
To install Pytesseract with Homebrew, first install `it`_ and follow the `steps`_.

.. _on the website: https://cloud.google.com/vision/docs/setup
.. _here: https://pypi.org/project/pytesseract/
.. _this website: https://tesseract-ocr.github.io/tessdoc/Installation.html
.. _it: https://brew.sh/
.. _steps: https://formulae.brew.sh/formula/tesseract

Installing zbar for MacOS and Linux
--------------------------------
To use the more powerful qr-code reading function of zbar additional dependencies
have to be installed (only for Mac OS and Linux. On Windows they come with the 
Python DLLs) These can be installed via the command line with the following
commands:

Mac OS X:

``brew install zbar``

Linux:

``sudo apt-get install libzbar0``

