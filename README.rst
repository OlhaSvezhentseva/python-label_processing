Collection Mining - Entomological Label Information Extraction
===================================================================

*A Python package for the Berlin Natural History Museum*

.. contents ::

Introduction
------------
This package is a comprehensive solution that seamlessly integrates a range of AI models and functional components, meticulously designed to facilitate the segmentation, classification, rotation, OCR, and clustering of entomology specimen labels. It serves as the foundational framework for the initial steps of information extraction, working in conjunction with the python-mfnb clustering package, which handles clustering in subsequent stages.

Moreover, the installation package includes three specialized TensorFlow classifiers, each thoughtfully adapted to accommodate the distinct styles of input labels. This additional functionality enhances the versatility of the package.

In addition to these core features, our package offers a set of carefully crafted scripts, designed to streamline label classification during the segmentation process, optimize image preprocessing before the application of Pytesseract or Google Vision OCR, and refine the postprocessing of OCR outputs to augment the clustering phase.

This comprehensive package allows to efficiently navigate the intricate landscape of entomology label processing while conserving valuable time and resources. It combines precision with versatility, making it an invaluable asset for data processing in the field of entomology.


Installation
------------
1. Clone python-mfnb from https://code.naturkundemuseum.berlin/collection-mining/label_processing.git .

   `git clone https://code.naturkundemuseum.berlin/collection-mining/label_processing.git`

2. cd in label_processing-main

   `cd <path to python-label_processing-main>`
   
3. Install with pip, which will automatically fetch the requirements.

   `pip install .`


Modules
-------
- label_processing:
   * segmentation_cropping
      Module containing all functions concerning the application of the segmentation 
      models, the classification of the cropped labels and the use of the predicted coordinates for cropping the labels.  


   * text_recognition
      Module containing the Pytesseract OCR parameters and image preprocessing to be performed on the _cropped jpg outputs from
      the segmentation_cropping.py module.


   * vision
      Module to call the Google Vision OCR API to be performed on the _cropped jpg outputs from
      the segmentation_cropping.py module.


   * utils
      Module containing general functions: raise an error if the _cropped jpg are not found in the given path, 
      generate a new filename and directory for the preprocessed images generated by Pytesseract, save the json files of 
      the transcription outputs generated by Google Vision (vision module) and Pytesseract (text_recognition module), 
      check and correct the NURI transcriptions in the json files to have a clean output.


   * backgroundcolor_detection
      Tries to recognise the background color of a picture and checks if it exceeds a given threshold. 
      If it exceeds the threshold it moves the corresponding pictures into a newly created directory.


   * clustering_preprocessing
      Module containing the preprocessing parameter for the OCR json file(s) before clustering. 
      It adds a specific identifier to each text outputs coming from the same picture.


   * rotator
      Classifier to detect orientation of image (0°, 90°, 180°, 270°) and to correct orientation.


   * tensorflow_classifier
      This module offers tools for image classification and organization.
      It involves loading a pre-trained image classifier model and using it to predict classes for images. 
      Subsequently, it facilitates the organization of these images into separate directories based on their predicted classes. 
      The module is designed to streamline the process of image classification and management.
      
      **Key Features:**
      
      1. **Loading a Trained Model:** The module provides a function to load a pre-trained Keras Sequential image classifier model from a specified file.

      2. **Predicting Classes:** Another function is available to predict the classes of images in a directory using the loaded model and generate a Pandas DataFrame containing the results.

      3. **Creating Separate Directories:** To organize the images, the module includes functionality to create separate directories for each class of images based on the predictions.

      4. **Renaming Images:** Images are renamed based on their predicted classes and saved in the corresponding directories.


   * accuracy_classifier
      Evaluate the performance of the tensorflow classifier.



- label_postprocessing:
   * utils
      Creates json files of the postprocessing modules' outputs.


   * vocabulary
      Extracts unique words from the transcripts and counts their occurrences.


   * nuri_postprocessing
      Creates two separated json files from the OCR outputs' json file.
      One for the NURIs and one of the rest of the transcription.



- label_evaluation:
   * redundancy
      Check the redundancy of a given transcription (groundtruth or OCR generated).

   
   * iou_scores
      Module containing the accuracy evaluation parameters of the segmentation model.
      Calculates IOU scores by comparing the ground truth and predicted segmentation coordinates.


   * evaluate_text
      Module containing the accuracy evaluation parameters of the OCR outputs.
      Calculates CER and WER by comparing the predicted and groundtruth transcriptions.



Scripts
-------
For usage information, run any of these scripts with the option --help.

* crop_seg.py
   Uses a segmentation-model to perform segmentation on jpg images, crop and classify them.

   **Inputs:**
      - the path to the directory of the input jpgs
      - the model used for the segmentation
      - the path to the directory in which the resulting crops and the csv will be stored

   **Outputs:**
      - the labels in the pictures are segmented and cropped out of the picture, becoming their own file named after 
        their jpg of origin and assigned class
      - the predicted segmentation outputs are also saved as a csv (filename, class, prediction score, coordinates)


* vision_api.py
   Performs the Google Vision OCR on the segmented labels by calling the API and returns it as a json file. 
   
   **Inputs:**
      - the path to the google credentials json file
      - the path to the directory of the input jpgs

   **Output:**
      - ocr results as a json file


* label_redundancy.py
   Module calculating labels' redundancy of a given text transcription (groundtruth or OCR generated).
   
   **Input:**
      - the path to the transcription dataset

   **Output:**
      - redundancy percentage of the dataset


* background_color.py
   Tries to recognize the background color of a picture before running the ocr. 
   
   **Input:**
      - directory which contains the cropped jpgs on which the ocr is supposed to be applied

   **Output:**
      - new directory with the pictures that exceed the given color threshold


* cluster_id.py
   Add unique identifiers to the pictures before clustering.
   
   **Inputs:**
      - path to the OCR output json file
      - path to where we want to save the preprocessed json file. Default is the user current working directory

   **Output:**
      - unique identifiers are added to the json file


* filter.py
   Responsible for filtering the ocr ouput according to 4 categories:
   nuris, empty transcripts, plausible output, nonsense output.
   Plausible output is corrected using regular expressions and is saveda as corrected_transcripts.json

   **Inputs:**
      - path to the OCR output json file

   **Output:**
      - one json file per category


* fix_spelling.py
   Checks if there are any spelling mistakes and fixes them.
   This is achieved by calculating Edit distance between words that appear fewer than 2 times with the 20 most frequent 
   words in the transcript. 
   If the Edit distance is lower/equal than a particular threshold, the word is substituted with a frequent word under 
   the assumption that this is the same word spelled correctly.

   **Inputs:**
      - path to json file
      - word frequency
      - distance
      - `vocabulary.csv`

   **Output:**
      - json file 

   1. Run `fix_spelling.py` to extract vocabulary (optionally) of the transcripts and correct spelling mistakes. Example:
         `python fix_spelling.py --transcripts corrected_transcripts.json --freq 20 --dist 0.34`

      transcripts: is the file you want correct transcripts from. It makes sense to use  
      `corrected_transcripts.json` that was created in the previous step (filter.py).

      freq: is the number of the most frequent words that low-frequence words will be compared to.

      dist: threshold for Edit distance. Distance less/equal than this value will be considered to be a small one, 
      so that the low-frequence word can be changed.
      
   2. If you already have `vocabulary.csv` file, then it should not be generated again, you may specify it:
          `python fix_spelling.py --transcripts corrected_transcripts.json --freq 20 --dist 0.34 --voc vocabulary.csv`


* ocr_accuracy.py
   Module containing the accuracy evaluation parameters of the OCR outputs.

   **Inputs:**
      - path to the ground truth dataset
      - path json file OCR output
      - target folder where the accuracy results are saved. Default is the user current working directory

   **Output:**
      - ocr accuracy scores (json file, plots)


* postprocessing_nuri.py
   Creates two separated json files from the OCR output json file.
   One for the NURIs and one of the rest of the transcription.

   **Inputs:**
      - path to the json file - OCR output
      - directory in which the json files will be saved. Default is the user current working directory
      - target folder where the accuracy results are saved. Default is the user current working directory

   **Output:**
      - json file - postprocessed ocr outputs


* process_ocr.py
   Filter the OCR ouputs according to 4 categories:nuris, empty transcripts, plausible output, nonsense output.
   Plausible outputs are corrected using regular expressions and is saved as corrected_transcripts.json.

   **Input:**
      - path to the json file - OCR output

   **Output:**
      - one json file per categories


* rotation.py
   Classifier to detect orientation of image (0°, 90°, 180°, 270°) and to correct orientation.

   **Inputs:**
      - directory where the rotated images will be stored. Default is the user current working directory
      - directory where the jpgs are stored

   **Output:**
      - rotated images in new directory


* segmentation_accuracy.py
   Evaluate segmentation model.

   **Inputs:**
      - path to the ground truth coordinates csv
      - path to the predicted coordinates csv
      - target folder where the iou accuracy results and plots are saved. Default is the user current working directory

   **Output:**
      - csv and box plots with accuracy scores
   

* tesseract_ocr.py
   Module containing the Pytesseract OCR parameters to be performed on the cropped jpg outputs.

   **Inputs:**
      - select whether verbose or quiet mode
      - optional argument: select thresholding 
      - optional argument: blocksize parameter for adaptive thresholding
      - optional argument: c_value parameter for adaptive thesholding
      - directory which contains the cropped jpgs on which the ocr is supposed to be applied

   **Outputs:**
      - preprocessed pictures
      - json file - OCR transcriptions


* image_classifier.py
   Execute the tensorflow_classifier.py module. Classify images into three categories: handwritten, typed and to_crop.

   **Inputs:**
      - path to the jpg images
      - path to the target directory where the classifed images should be saved

   **Output:**
      - classified images into new target directories


* evaluation_classifier.py
   Execute the accuracy_classifier.py module. Evaluate the performance of the classification model.

   **Inputs:**
      - path to the ground truth dataframe
      - path to the target directory where the confusion matrix should be saved

   **Outputs:**
      - accuracy metrics
      - confusion matrix



Input preparation
-----------------
**The modules are best to be performed on jpg images of labels from entomology databases such as:**
   - `AntWeb`_
   - `Bees&Bytes`_
   - LEP_PHIL - pictures of specimens from the Philippines (by Théo Leger)
   - `Atlas of Living Australia`_


**In terms of data acquisition, the following standards are recommended to optimize the outputs:**

- The pictures quality should be standardized and uniform as much as possible, preferably using macro photography, the .jpg format and 300 DPI resolution.
- If there are multiple labels in one picture, they should be clearly separated from one another without overlapping. The text in the label should be aligned horizontally.
- If possible, the specimen shouldn't be present in the picture with the labels.
- If the labels in the different pictures are similar (same colours and/or same nature/content), they should always be placed the same way at the same spot from one picture to another. *ex: label with location always bottom right, collection number top left, taxonomy top right etc...*
- A black background like in LEP_PHIL is prefered, but a white background is also acceptable.


.. _AntWeb: https://www.antweb.org/
.. _Bees&Bytes: https://www.zooniverse.org/projects/mfnberlin/bees-and-bytes  
.. _Atlas of Living Australia: https://www.ala.org.au/


Setting up google cloud vision API and getting credentials
----------------------------------------------------------
- In order to use the google API you need to create a Google account and set it up for Vision.
- How to setup your Google Cloud Vision is explained `on the website`_.
- You then need to retrieve your credentials json (everything is explained in the provided link).
- The credentials json file should then be provided as an input in the `vision.py` script.


Installing Pytesseract for MacOS"""
Responsible for filtering the OCR ouputs according to 4 categories:nuris, empty transcripts, plausible output, nonsense output.
Plausible outputs are corrected using regular expressions and is saved as corrected_transcripts.json.
"""
--------------------------------
Informations about Pytesseract can be found `here`_ or `this website`_.
To install Pytesseract with Homebrew, first install `it`_ and follow the `steps`_.

.. _on the website: https://cloud.google.com/vision/docs/setup
.. _here: https://pypi.org/project/pytesseract/
.. _this website: https://tesseract-ocr.github.io/tessdoc/Installation.html
.. _it: https://brew.sh/
.. _steps: https://formulae.brew.sh/formula/tesseract


Installing zbar for MacOS and Linux
-----------------------------------
To use the more powerful qr-code reading function of zbar additional dependencies
have to be installed (only for Mac OS and Linux. On Windows they come with the 
Python DLLs) These can be installed via the command line with the following
commands:

Mac OS X:

``brew install zbar``

Linux:

``sudo apt-get install libzbar0``


Contacts
--------

Margot Belot margot.belot@mfn.berlin

Olha Svezhentseva Olha.Svezhentseva@mfn.berlin

Leonardo Preuss preuss.leonardo@gmail.com

